{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q\n",
    "!pip install datasets -q\n",
    "!pip install tiktoken -q \n",
    "!pip install wandb -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "#wandb_project = 'OpenWebText-GPT2'\n",
    "#wandb.init(project=wandb_project, entity='veetil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED 768\n",
      "FF_EXP 4\n",
      "LAYERS 12\n",
      "FC_DROPOUT 0.1\n",
      "bias True\n",
      "VOCAB 50304\n",
      "N_HEAD 12\n",
      "ATTN_DROPOUT 0.1\n",
      "RESID_DROPOUT 0.1\n",
      "BLOCK_SIZE 1024\n",
      "EPS 1e-05\n",
      "EMBED_DROPOUT 0.1\n",
      "MAX_POS_EMBED 1024\n",
      "DROPOUT 0.1\n",
      "SWIGLU 0\n",
      "RMS_NORM 1\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "import config\n",
    "config_ = config.read_config()\n",
    "config.print_config(config_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED 768\n",
      "FF_EXP 4\n",
      "LAYERS 12\n",
      "FC_DROPOUT 0.1\n",
      "bias True\n",
      "VOCAB 50304\n",
      "N_HEAD 12\n",
      "ATTN_DROPOUT 0.1\n",
      "RESID_DROPOUT 0.1\n",
      "BLOCK_SIZE 1024\n",
      "EPS 1e-05\n",
      "EMBED_DROPOUT 0.1\n",
      "MAX_POS_EMBED 1024\n",
      "DROPOUT 0.1\n",
      "SWIGLU 0\n",
      "RMS_NORM 1\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "config_ = config.read_config()\n",
    "config.print_config(config_)\n",
    "## instatitate gpt 2 from pytorch and print its architecture\n",
    "#from transformers import GPT2Model\n",
    "#model = GPT2Model.from_pretrained('gpt2')\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "Instantiating RMSNorm 1\n",
      "Instantiating RMSNorm 2\n",
      "Instantiating regular FFN\n",
      "GPT2Model(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50304, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): RMSNorm()\n",
      "        (attn): SelfAttentionLayer(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): RMSNorm()\n",
      "        (mlp): FeedForwardLayer(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from gpt import GPT2Model\n",
    "model = GPT2Model(config_)\n",
    "print(model)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[30760, 49026, 38663,  ..., 40089, 24822, 43044],\n",
      "        [44664, 31416,  6269,  ..., 38835, 48152, 12529]])\n"
     ]
    }
   ],
   "source": [
    "## generate x as sequence of tokens from 1 to VOCAB_SIZE of length 10\n",
    "x = torch.randint(0, config_.VOCAB, (2, config_.BLOCK_SIZE))\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.6752, -0.0125,  0.3488,  ..., -0.2893,  0.1451, -0.4160]],\n",
       " \n",
       "         [[ 0.9384,  0.0162,  1.0120,  ..., -1.2838,  0.1468,  0.1546]]],\n",
       "        grad_fn=<UnsafeViewBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.attn.c_proj.weight\n",
      "transformer.h.0.mlp.c_proj.weight\n",
      "transformer.h.1.attn.c_proj.weight\n",
      "transformer.h.1.mlp.c_proj.weight\n",
      "transformer.h.2.attn.c_proj.weight\n",
      "transformer.h.2.mlp.c_proj.weight\n",
      "transformer.h.3.attn.c_proj.weight\n",
      "transformer.h.3.mlp.c_proj.weight\n",
      "transformer.h.4.attn.c_proj.weight\n",
      "transformer.h.4.mlp.c_proj.weight\n",
      "transformer.h.5.attn.c_proj.weight\n",
      "transformer.h.5.mlp.c_proj.weight\n",
      "transformer.h.6.attn.c_proj.weight\n",
      "transformer.h.6.mlp.c_proj.weight\n",
      "transformer.h.7.attn.c_proj.weight\n",
      "transformer.h.7.mlp.c_proj.weight\n",
      "transformer.h.8.attn.c_proj.weight\n",
      "transformer.h.8.mlp.c_proj.weight\n",
      "transformer.h.9.attn.c_proj.weight\n",
      "transformer.h.9.mlp.c_proj.weight\n",
      "transformer.h.10.attn.c_proj.weight\n",
      "transformer.h.10.mlp.c_proj.weight\n",
      "transformer.h.11.attn.c_proj.weight\n",
      "transformer.h.11.mlp.c_proj.weight\n"
     ]
    }
   ],
   "source": [
    "for pn, p in model.named_parameters():\n",
    "    if pn.endswith('c_proj.weight'):\n",
    "        print(pn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_ARM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
